---
sidebar_position: 1
---

# Language Models

Language models are artificial intelligence models that have been trained to understand human language. They are trained to do this through the next word prediction task. Given an incomplete sentence, the language model is required to complete the sentence. For example, if you give the sentence **The cat sat on the** the language model would complete it with **The cat sat on the table** or **The cat sat on the mat.** Their prediction is probabilistic in nature, so it varies for every trial. It depends on what they were trained on.

In simple terms, language models can be thought of as glorified autocomplete.

## Large Language Models

Large Language Models (LLMs) are a type of language model that have been trained on a large [corpus of text](https://en.wikipedia.org/wiki/Text_corpus), typically the entire internet. They have a large number of [parameters](https://deepchecks.com/glossary/model-parameters/). A typical language model may have thousands of parameters, while a Large Language Model has millions to billions of parameters.

Their performance usually increases as their parameters and training set increase. This is known as the [neural scaling law](https://en.wikipedia.org/wiki/Neural_scaling_law).